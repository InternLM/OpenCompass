_meta_template=dict(
    round=[
        dict(begin='\n<|im_start|>user\n',
            end='<|im_end|>',
            role='HUMAN'),
        dict(begin='\n<|im_start|>assistant\n',
            end='<|im_end|>',
            generate=True,
            role='BOT'),
        ])
api_meta_template=dict(
    reserved_roles=[
        dict(api_role='SYSTEM',
            role='SYSTEM'),
        ],
    round=[
        dict(api_role='HUMAN',
            role='HUMAN'),
        dict(api_role='BOT',
            generate=True,
            role='BOT'),
        ])
datasets=[
    dict(abbr='COREV2_6A_',
        eval_cfg=dict(
            evaluator=dict(
                prompt_template=dict(
                    template=dict(
                        round=[
                            dict(prompt='{prefix}问题: <问题开始> {question} <问题结束>\n\n回答 1: <回答 1 开始> {prediction} <回答 1 结束>\n\n回答 2: <回答 2 开始> {prediction2} <回答 2 结束>\n\n{suffix}',
                                role='HUMAN'),
                            ]),
                    type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
                random_order=True,
                type='opencompass.openicl.icl_evaluator.LMEvaluator'),
            pred_role='BOT'),
        infer_cfg=dict(
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                template=dict(
                    round=[
                        dict(prompt='{question}',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                type='opencompass.openicl.icl_retriever.ZeroRetriever')),
        name='COREV2_6A_',
        path='./data/subjective/',
        reader_cfg=dict(
            input_columns=[
                'question',
                'prefix',
                'suffix',
                ],
            output_column='judge'),
        type='opencompass.datasets.Corev2Dataset'),
    ]
eval=dict(
    partitioner=dict(
        mode='allpair',
        type='opencompass.partitioners.sub_naive.SubjectiveNaivePartitioner'),
    runner=dict(
        max_num_workers=256,
        partition='llmeval',
        quotatype='auto',
        task=dict(
            judge_cfg=dict(
                abbr='qwen-7b-chat-hf',
                batch_size=8,
                max_out_len=2048,
                max_seq_len=2048,
                meta_template=dict(
                    round=[
                        dict(begin='\n<|im_start|>user\n',
                            end='<|im_end|>',
                            role='HUMAN'),
                        dict(begin='\n<|im_start|>assistant\n',
                            end='<|im_end|>',
                            generate=True,
                            role='BOT'),
                        ]),
                model_kwargs=dict(
                    device_map='auto',
                    trust_remote_code=True),
                pad_token_id=151643,
                path='Qwen/Qwen-7B-Chat',
                run_cfg=dict(
                    num_gpus=1,
                    num_procs=1),
                tokenizer_kwargs=dict(
                    padding_side='left',
                    truncation_side='left',
                    trust_remote_code=True,
                    use_fast=False),
                tokenizer_path='Qwen/Qwen-7B-Chat',
                type='opencompass.models.HuggingFaceCausalLM'),
            type='opencompass.tasks.subjective_eval.SubjectiveEvalTask'),
        type='opencompass.runners.SlurmSequentialRunner'))
hf_baichuan2_7b=[
    dict(abbr='baichuan2-7b-chat-hf',
        batch_size=8,
        max_out_len=2048,
        max_seq_len=2048,
        meta_template=dict(
            round=[
                dict(begin='<reserved_106>',
                    role='HUMAN'),
                dict(begin='<reserved_107>',
                    generate=True,
                    role='BOT'),
                ]),
        model_kwargs=dict(
            device_map='auto',
            trust_remote_code=True),
        path='baichuan-inc/Baichuan2-7B-Chat',
        run_cfg=dict(
            num_gpus=1,
            num_procs=1),
        tokenizer_kwargs=dict(
            padding_side='left',
            truncation_side='left',
            trust_remote_code=True,
            use_fast=False),
        tokenizer_path='baichuan-inc/Baichuan2-7B-Chat',
        type='opencompass.models.HuggingFaceCausalLM'),
    ]
hf_chatglm3_6b=[
    dict(abbr='chatglm3-6b-hf',
        batch_size=1,
        max_out_len=2048,
        max_seq_len=2048,
        meta_template=dict(
            round=[
                dict(api_role='HUMAN',
                    role='HUMAN'),
                dict(api_role='BOT',
                    generate=True,
                    role='BOT'),
                ]),
        model_kwargs=dict(
            device_map='auto',
            trust_remote_code=True),
        path='THUDM/chatglm3-6b',
        run_cfg=dict(
            num_gpus=1,
            num_procs=1),
        tokenizer_kwargs=dict(
            padding_side='left',
            truncation_side='left',
            trust_remote_code=True),
        tokenizer_path='THUDM/chatglm3-6b',
        type='opencompass.models.HuggingFaceChatGLM3'),
    ]
hf_internlm_chat_20b=[
    dict(abbr='internlm-chat-20b-hf',
        batch_size=8,
        max_out_len=2048,
        max_seq_len=2048,
        meta_template=dict(
            round=[
                dict(begin='<|User|>:你是一个图文创作助手，可以辅助创作图文并茂的文章，请你根据下面给定的要求进行创作或回复长文章：\n',
                    end='<eoh>\n',
                    role='HUMAN'),
                dict(begin='<|Bot|>:',
                    end='<eoa>\n',
                    generate=True,
                    role='BOT'),
                ]),
        model_kwargs=dict(
            device_map='auto',
            trust_remote_code=True),
        path='internlm/internlm-chat-20b',
        run_cfg=dict(
            num_gpus=2,
            num_procs=1),
        tokenizer_kwargs=dict(
            padding_side='left',
            truncation_side='left',
            trust_remote_code=True,
            use_fast=False),
        tokenizer_path='internlm/internlm-chat-20b',
        type='opencompass.models.HuggingFaceCausalLM'),
    ]
hf_internlm_chat_7b=[
    dict(abbr='internlm-chat-7b-hf',
        batch_size=8,
        max_out_len=2048,
        max_seq_len=2048,
        meta_template=dict(
            round=[
                dict(begin='<|User|>:',
                    end='<eoh>\n',
                    role='HUMAN'),
                dict(begin='<|Bot|>:',
                    end='<eoa>\n',
                    generate=True,
                    role='BOT'),
                ]),
        model_kwargs=dict(
            device_map='auto',
            trust_remote_code=True),
        path='internlm/internlm-chat-7b',
        run_cfg=dict(
            num_gpus=1,
            num_procs=1),
        tokenizer_kwargs=dict(
            padding_side='left',
            truncation_side='left',
            trust_remote_code=True,
            use_fast=False),
        tokenizer_path='internlm/internlm-chat-7b',
        type='opencompass.models.HuggingFaceCausalLM'),
    ]
hf_qwen_14b_chat=[
    dict(abbr='qwen-14b-chat-hf',
        batch_size=8,
        max_out_len=2048,
        max_seq_len=2048,
        meta_template=dict(
            round=[
                dict(begin='\n<|im_start|>user\n',
                    end='<|im_end|>',
                    role='HUMAN'),
                dict(begin='\n<|im_start|>assistant\n',
                    end='<|im_end|>',
                    generate=True,
                    role='BOT'),
                ]),
        model_kwargs=dict(
            device_map='auto',
            trust_remote_code=True),
        pad_token_id=151643,
        path='Qwen/Qwen-14B-Chat',
        run_cfg=dict(
            num_gpus=1,
            num_procs=1),
        tokenizer_kwargs=dict(
            padding_side='left',
            truncation_side='left',
            trust_remote_code=True,
            use_fast=False),
        tokenizer_path='Qwen/Qwen-14B-Chat',
        type='opencompass.models.HuggingFaceCausalLM'),
    ]
hf_qwen_7b_chat=[
    dict(abbr='qwen-7b-chat-hf',
        batch_size=8,
        max_out_len=2048,
        max_seq_len=2048,
        meta_template=dict(
            round=[
                dict(begin='\n<|im_start|>user\n',
                    end='<|im_end|>',
                    role='HUMAN'),
                dict(begin='\n<|im_start|>assistant\n',
                    end='<|im_end|>',
                    generate=True,
                    role='BOT'),
                ]),
        model_kwargs=dict(
            device_map='auto',
            trust_remote_code=True),
        pad_token_id=151643,
        path='Qwen/Qwen-7B-Chat',
        run_cfg=dict(
            num_gpus=1,
            num_procs=1),
        tokenizer_kwargs=dict(
            padding_side='left',
            truncation_side='left',
            trust_remote_code=True,
            use_fast=False),
        tokenizer_path='Qwen/Qwen-7B-Chat',
        type='opencompass.models.HuggingFaceCausalLM'),
    ]
infer=dict(
    partitioner=dict(
        type='opencompass.partitioners.NaivePartitioner'),
    runner=dict(
        max_num_workers=256,
        partition='llmeval',
        quotatype='auto',
        task=dict(
            type='opencompass.tasks.OpenICLInferTask'),
        type='opencompass.runners.SlurmSequentialRunner'))
judge_model=dict(
    abbr='qwen-7b-chat-hf',
    batch_size=8,
    max_out_len=2048,
    max_seq_len=2048,
    meta_template=dict(
        round=[
            dict(begin='\n<|im_start|>user\n',
                end='<|im_end|>',
                role='HUMAN'),
            dict(begin='\n<|im_start|>assistant\n',
                end='<|im_end|>',
                generate=True,
                role='BOT'),
            ]),
    model_kwargs=dict(
        device_map='auto',
        trust_remote_code=True),
    pad_token_id=151643,
    path='Qwen/Qwen-7B-Chat',
    run_cfg=dict(
        num_gpus=1,
        num_procs=1),
    tokenizer_kwargs=dict(
        padding_side='left',
        truncation_side='left',
        trust_remote_code=True,
        use_fast=False),
    tokenizer_path='Qwen/Qwen-7B-Chat',
    type='opencompass.models.HuggingFaceCausalLM')
models=[
    dict(abbr='baichuan2-7b-chat-hf',
        batch_size=8,
        max_out_len=2048,
        max_seq_len=2048,
        meta_template=dict(
            round=[
                dict(begin='<reserved_106>',
                    role='HUMAN'),
                dict(begin='<reserved_107>',
                    generate=True,
                    role='BOT'),
                ]),
        model_kwargs=dict(
            device_map='auto',
            trust_remote_code=True),
        path='baichuan-inc/Baichuan2-7B-Chat',
        run_cfg=dict(
            num_gpus=1,
            num_procs=1),
        tokenizer_kwargs=dict(
            padding_side='left',
            truncation_side='left',
            trust_remote_code=True,
            use_fast=False),
        tokenizer_path='baichuan-inc/Baichuan2-7B-Chat',
        type='opencompass.models.HuggingFaceCausalLM'),
    dict(abbr='qwen-7b-chat-hf',
        batch_size=8,
        max_out_len=2048,
        max_seq_len=2048,
        meta_template=dict(
            round=[
                dict(begin='\n<|im_start|>user\n',
                    end='<|im_end|>',
                    role='HUMAN'),
                dict(begin='\n<|im_start|>assistant\n',
                    end='<|im_end|>',
                    generate=True,
                    role='BOT'),
                ]),
        model_kwargs=dict(
            device_map='auto',
            trust_remote_code=True),
        pad_token_id=151643,
        path='Qwen/Qwen-7B-Chat',
        run_cfg=dict(
            num_gpus=1,
            num_procs=1),
        tokenizer_kwargs=dict(
            padding_side='left',
            truncation_side='left',
            trust_remote_code=True,
            use_fast=False),
        tokenizer_path='Qwen/Qwen-7B-Chat',
        type='opencompass.models.HuggingFaceCausalLM'),
    ]
subjective_datasets=[
    dict(abbr='COREV2_6A_',
        eval_cfg=dict(
            evaluator=dict(
                prompt_template=dict(
                    template=dict(
                        round=[
                            dict(prompt='{prefix}问题: <问题开始> {question} <问题结束>\n\n回答 1: <回答 1 开始> {prediction} <回答 1 结束>\n\n回答 2: <回答 2 开始> {prediction2} <回答 2 结束>\n\n{suffix}',
                                role='HUMAN'),
                            ]),
                    type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
                random_order=True,
                type='opencompass.openicl.icl_evaluator.LMEvaluator'),
            pred_role='BOT'),
        infer_cfg=dict(
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                template=dict(
                    round=[
                        dict(prompt='{question}',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                type='opencompass.openicl.icl_retriever.ZeroRetriever')),
        name='COREV2_6A_',
        path='./data/subjective/',
        reader_cfg=dict(
            input_columns=[
                'question',
                'prefix',
                'suffix',
                ],
            output_column='judge'),
        type='opencompass.datasets.Corev2Dataset'),
    ]
summarizer=dict(
    judge_method='gpt4',
    type='opencompass.summarizers.Corev2Summarizer')
work_dir='./trash/corev2/subject/20231208_194351'