Among total 8 predictions, there are 2 predictions totally same, which are removed!
03/08 16:56:39 - OpenCompass - [4m[97mINFO[0m - Task [baichuan_qwen/creationv2_zh_test]
03/08 16:56:39 - OpenCompass - [4m[97mINFO[0m - time elapsed: 25.66s
srun: job 3458644 queued and waiting for resources
srun: job 3458644 has been allocated resources
srun: Job 3458644 scheduled successfully!
Current QUOTA_TYPE is [spot], which means the job has occupied quota in SPOT_TOTAL under your partition.
[NOTE]: This job MAY BE PREEMPTED by other jobs of reserved QUOTA_TYPE.
[NOTE]: phx_priority has NO effect on job of spot quota type.

/mnt/petrelfs/caomaosong/miniconda3/envs/opencompass/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
A new version of the following files was downloaded from https://huggingface.co/THUDM/chatglm3-6b:
- tokenization_chatglm.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
A new version of the following files was downloaded from https://huggingface.co/THUDM/chatglm3-6b:
- configuration_chatglm.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.
A new version of the following files was downloaded from https://huggingface.co/THUDM/chatglm3-6b:
- modeling_chatglm.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.

Downloading shards:   0%|          | 0/7 [00:00<?, ?it/s]
Downloading shards: 100%|██████████| 7/7 [00:00<00:00, 748.89it/s]

Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]
Loading checkpoint shards:  14%|█▍        | 1/7 [00:00<00:05,  1.14it/s]
Loading checkpoint shards:  29%|██▊       | 2/7 [00:01<00:04,  1.16it/s]
Loading checkpoint shards:  43%|████▎     | 3/7 [00:02<00:03,  1.17it/s]
Loading checkpoint shards:  57%|█████▋    | 4/7 [00:03<00:02,  1.18it/s]
Loading checkpoint shards:  71%|███████▏  | 5/7 [00:04<00:01,  1.19it/s]
Loading checkpoint shards:  86%|████████▌ | 6/7 [00:05<00:00,  1.20it/s]
Loading checkpoint shards: 100%|██████████| 7/7 [00:05<00:00,  1.35it/s]
Loading checkpoint shards: 100%|██████████| 7/7 [00:05<00:00,  1.25it/s]

Flattening the indices:   0%|          | 0/6 [00:00<?, ? examples/s]
Flattening the indices: 100%|██████████| 6/6 [00:00<00:00, 1465.86 examples/s]
[2024-03-08 16:56:25,789] [opencompass.openicl.icl_inferencer.icl_gen_inferencer] [INFO] Starting inference process...

  0%|          | 0/6 [00:00<?, ?it/s]Both `max_new_tokens` (=2048) and `max_length`(=8192) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)

 17%|█▋        | 1/6 [00:03<00:17,  3.46s/it]Both `max_new_tokens` (=2048) and `max_length`(=8192) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)

 33%|███▎      | 2/6 [00:05<00:09,  2.44s/it]Both `max_new_tokens` (=2048) and `max_length`(=8192) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)

 50%|█████     | 3/6 [00:07<00:07,  2.41s/it]Both `max_new_tokens` (=2048) and `max_length`(=8192) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)

 67%|██████▋   | 4/6 [00:09<00:04,  2.23s/it]Both `max_new_tokens` (=2048) and `max_length`(=8192) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)

 83%|████████▎ | 5/6 [00:12<00:02,  2.42s/it]Both `max_new_tokens` (=2048) and `max_length`(=8192) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)

100%|██████████| 6/6 [00:13<00:00,  1.95s/it]
100%|██████████| 6/6 [00:13<00:00,  2.22s/it]
