alignbench_datasets=[
    dict(abbr='alignment_bench',
        alignment_bench_config_name='multi-dimension',
        alignment_bench_config_path='data/subjective/alignment_bench/config',
        eval_cfg=dict(
            evaluator=dict(
                prompt_template=dict(
                    template=dict(
                        round=[
                            dict(prompt='{critiquellm_prefix}[助手的答案开始]\n{prediction}\n[助手的答案结束]\n',
                                role='HUMAN'),
                            ]),
                    type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
                type='opencompass.openicl.icl_evaluator.LMEvaluator'),
            pred_role='BOT'),
        infer_cfg=dict(
            inferencer=dict(
                max_out_len=2048,
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                template=dict(
                    round=[
                        dict(prompt='{question}',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                type='opencompass.openicl.icl_retriever.ZeroRetriever')),
        mode='singlescore',
        name='alignment_bench',
        path='data/subjective/alignment_bench',
        reader_cfg=dict(
            input_columns=[
                'question',
                'capability',
                'critiquellm_prefix',
                ],
            output_column='judge'),
        summarizer=dict(
            judge_type='general',
            type='opencompass.summarizers.AlignmentBenchSummarizer'),
        type='opencompass.datasets.AlignmentBenchDataset'),
    ]
alpacav2=[
    dict(abbr='alpaca_eval',
        base_models=[
            dict(abbr='gpt4-turbo',
                type='opencompass.models.openai_api.OpenAI'),
            ],
        eval_cfg=dict(
            evaluator=dict(
                prompt_template=dict(
                    template=dict(
                        begin=[
                            dict(fallback_role='HUMAN',
                                prompt='You are a highly efficient assistant, who evaluates and selects the best large language model (LLMs) based on the quality of their responses to a given instruction. This process will be used to create a leaderboard reflecting the most accurate and human-preferred answers.',
                                role='SYSTEM'),
                            ],
                        round=[
                            dict(prompt='\nI require a leaderboard for various large language models. I\'ll provide you with prompts given to these models and their corresponding outputs. Your task is to assess these responses, and select the model that produces the best output from a human perspective.\n\n## Instruction\n\n{\n    "instruction": "{question}",\n}\n\n## Model Outputs\n\nHere are the unordered outputs from the models. Each output is associated with a specific model, identified by a unique model identifier.\n\n{\n    {\n        "model_identifier": "m",\n        "output": "{prediction}"\n    },\n    {\n        "model_identifier": "M",\n        "output": "{prediction2}"\n    }\n}\n\n## Task\n\nEvaluate the models based on the quality and relevance of their outputs, and select the model that generated the best output. Answer by providing the model identifier of the best model. We will use your output as the name of the best model, so make sure your output only contains one of the following model identifiers and nothing else (no quotes, no spaces, no new lines, ...): m or M.\n\n## Best Model Identifier\n',
                                role='HUMAN'),
                            ]),
                    type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
                type='opencompass.openicl.icl_evaluator.LMEvaluator'),
            pred_role='BOT'),
        infer_cfg=dict(
            inferencer=dict(
                max_out_len=4096,
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                template=dict(
                    round=[
                        dict(prompt='{question}',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                type='opencompass.openicl.icl_retriever.ZeroRetriever')),
        infer_order='random',
        mode='m2n',
        name='alpaca_eval',
        path='./data/subjective/alpaca_eval',
        reader_cfg=dict(
            input_columns=[
                'question',
                ],
            output_column='judge'),
        summarizer=dict(
            judge_type='v2',
            type='opencompass.summarizers.AlpacaSummarizer'),
        type='opencompass.datasets.SubjectiveCmpDataset'),
    ]
api_meta_template=dict(
    round=[
        dict(api_role='HUMAN',
            role='HUMAN'),
        dict(api_role='BOT',
            generate=True,
            role='BOT'),
        ])
arenahard_datasets=[
    dict(abbr='arenahard',
        base_models=[
            dict(abbr='gpt4-0314',
                type='opencompass.models.openai_api.OpenAI'),
            ],
        eval_cfg=dict(
            evaluator=dict(
                prompt_template=dict(
                    template=dict(
                        begin=[
                            dict(fallback_role='HUMAN',
                                prompt='Please act as an impartial judge and evaluate the quality of the responses provided by two AI assistants to the user prompt displayed below. You will be given assistant A\'s answer and assistant B\'s answer. Your job is to evaluate which assistant\'s answer is better.\n\nBegin your evaluation by generating your own answer to the prompt. You must provide your answers before judging any answers.\n\nWhen evaluating the assistants\' answers, compare both assistants\' answers with your answer. You must identify and correct any mistakes or inaccurate information.\n\nThen consider if the assistant\'s answers are helpful, relevant, and concise. Helpful means the answer correctly responds to the prompt or follows the instructions. Note when user prompt has any ambiguity or more than one interpretation, it is more helpful and appropriate to ask for clarifications or more information from the user than providing an answer based on assumptions. Relevant means all parts of the response closely connect or are appropriate to what is being asked. Concise means the response is clear and not verbose or excessive.\n\nThen consider the creativity and novelty of the assistant\'s answers when needed. Finally, identify any missing important information in the assistants\' answers that would be beneficial to include when responding to the user prompt.\n\nAfter providing your explanation, you must output only one of the following choices as your final verdict with a label:\n\n1. Assistant A is significantly better: [[A>>B]]\n2. Assistant A is slightly better: [[A>B]]\n3. Tie, relatively the same: [[A=B]]\n4. Assistant B is slightly better: [[B>A]]\n5. Assistant B is significantly better: [[B>>A]]\n\nExample output: "My final verdict is tie: [[A=B]]".',
                                role='SYSTEM'),
                            ],
                        round=[
                            dict(prompt="<|User Prompt|>\n{question}\n\n<|The Start of Assistant A's Answer|>\n{prediction}\n<|The End of Assistant A's Answer|>\n\n<|The Start of Assistant B's Answer|>\n{prediction2}\n<|The End of Assistant B's Answer|>",
                                role='HUMAN'),
                            ]),
                    type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
                type='opencompass.openicl.icl_evaluator.LMEvaluator'),
            pred_role='BOT'),
        infer_cfg=dict(
            inferencer=dict(
                max_out_len=4096,
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                template=dict(
                    round=[
                        dict(prompt='{question}',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                type='opencompass.openicl.icl_retriever.ZeroRetriever')),
        infer_order='double',
        mode='m2n',
        name='question',
        path='./data/subjective/arena_hard',
        reader_cfg=dict(
            input_columns=[
                'question',
                ],
            output_column='judge'),
        summarizer=dict(
            type='opencompass.summarizers.ArenaHardSummarizer'),
        type='opencompass.datasets.ArenaHardDataset'),
    ]
compassarena_datasets=[
    dict(abbr='compassarena_language',
        base_models=[
            dict(abbr='gpt4-turbo',
                type='opencompass.models.openai_api.OpenAI'),
            ],
        eval_cfg=dict(
            evaluator=dict(
                prompt_template=dict(
                    template=dict(
                        round=[
                            dict(prompt='\n请根据提供的 评分要求，用户问题 以及 相应的两个回答（回答1，回答2），判断两个回答中哪一个更好。\n评分要求（重要性依次递减）:\n1. 在有明确的参考答案的情况下，越贴近参考答案或表明了参考答案的意思的回答越好。\n2. 更好的回答在语言表达上更流畅，更加符合与人类对话的习惯，包括语气、情调等\n3. 在都准确答对问题的前提下，更好的回答能进行额外补充，且补充的内容准确无误。\n\n[用户问题]\n{question}\n\n[参考答案]\n{ref}\n\n\n[回答1开始]\n{prediction}\n[回答1结束]\n\n[回答2开始]\n{prediction2}\n[回答2结束]\n\n根据评分要求，在以下 3 个选项中做出选择:\nA. 回答1更好\nB. 回答2更好\nC. 回答1、2平局\n并提供你的解释原因。\n\n如果你认为回答1更好，你的输出应形如：\n选择：A\n原因：blahblah blahblah\n\n\n如果你认为回答2更好，你的输出应形如：\n选择：B\n原因：blahblah blahblah\n\n\n如果你认为回答1、2打成平手，你的输出应形如：\n选择：C\n原因：blahblah blahblah\n\n',
                                role='HUMAN'),
                            ]),
                    type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
                type='opencompass.openicl.icl_evaluator.LMEvaluator'),
            pred_role='BOT'),
        infer_cfg=dict(
            inferencer=dict(
                max_out_len=2048,
                max_seq_len=4096,
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                template=dict(
                    round=[
                        dict(prompt='{question}',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                type='opencompass.openicl.icl_retriever.ZeroRetriever')),
        infer_order='double',
        mode='m2n',
        name='language',
        path='data/subjective/compass_arena',
        reader_cfg=dict(
            input_columns=[
                'question',
                'ref',
                ],
            output_column='judge'),
        summarizer=dict(
            summary_type='half_add',
            type='opencompass.summarizers.CompassArenaSummarizer'),
        type='opencompass.datasets.CompassArenaDataset'),
    dict(abbr='compassarena_knowledge',
        base_models=[
            dict(abbr='gpt4-turbo',
                type='opencompass.models.openai_api.OpenAI'),
            ],
        eval_cfg=dict(
            evaluator=dict(
                prompt_template=dict(
                    template=dict(
                        round=[
                            dict(prompt='\n请根据提供的 评分要求，用户问题，参考答案 以及 相应的两个回答（回答1，回答2），判断两个回答中哪一个更好。\n评分要求（重要性依次递减）:\n1. 更好的回答能与参考答案吻合或表明参考答案的意思。\n2. 在都准确答对问题的前提下，更好的回答能对知识点进行额外补充，且补充的知识准确无误。\n3. 更好的回答更加符合与人类对话的习惯，包括语气、情调等。\n\n[用户问题]\n{question}\n\n[参考答案]\n{ref}\n\n\n[回答1开始]\n{prediction}\n[回答1结束]\n\n[回答2开始]\n{prediction2}\n[回答2结束]\n\n根据评分要求，在以下 3 个选项中做出选择:\nA. 回答1更好\nB. 回答2更好\nC. 回答1、2平局\n并提供你的解释原因。\n\n如果你认为回答1更好，你的输出应形如：\n选择：A\n原因：blahblah blahblah\n\n\n如果你认为回答2更好，你的输出应形如：\n选择：B\n原因：blahblah blahblah\n\n\n如果你认为回答1、2打成平手，你的输出应形如：\n选择：C\n原因：blahblah blahblah\n\n',
                                role='HUMAN'),
                            ]),
                    type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
                type='opencompass.openicl.icl_evaluator.LMEvaluator'),
            pred_role='BOT'),
        infer_cfg=dict(
            inferencer=dict(
                max_out_len=2048,
                max_seq_len=4096,
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                template=dict(
                    round=[
                        dict(prompt='{question}',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                type='opencompass.openicl.icl_retriever.ZeroRetriever')),
        infer_order='double',
        mode='m2n',
        name='knowledge',
        path='data/subjective/compass_arena',
        reader_cfg=dict(
            input_columns=[
                'question',
                'ref',
                ],
            output_column='judge'),
        summarizer=dict(
            summary_type='half_add',
            type='opencompass.summarizers.CompassArenaSummarizer'),
        type='opencompass.datasets.CompassArenaDataset'),
    dict(abbr='compassarena_reason_v2',
        base_models=[
            dict(abbr='gpt4-turbo',
                type='opencompass.models.openai_api.OpenAI'),
            ],
        eval_cfg=dict(
            evaluator=dict(
                prompt_template=dict(
                    template=dict(
                        round=[
                            dict(prompt='\n请根据提供的 评分要求，用户问题，参考答案 以及 相应的两个回答（回答1，回答2），判断两个回答中哪一个更好。\n评分要求（重要性依次递减）:\n1. 更好的回答的答案能和参考答案一致。\n2. 若两个回答的答案都与参考答案不一致，则更好的回答的推理过程应更加合理。\n3. 更好的回答更加符合与人类对话的习惯，包括语气、情调等。\n\n[用户问题]\n{question}\n\n[参考答案]\n{ref}\n\n\n[回答1开始]\n{prediction}\n[回答1结束]\n\n[回答2开始]\n{prediction2}\n[回答2结束]\n\n根据评分要求，在以下 3 个选项中做出选择:\nA. 回答1更好\nB. 回答2更好\nC. 回答1、2平局\n并提供你的解释原因。\n\n如果你认为回答1更好，你的输出应形如：\n选择：A\n原因：blahblah blahblah\n\n\n如果你认为回答2更好，你的输出应形如：\n选择：B\n原因：blahblah blahblah\n\n\n如果你认为回答1、2打成平手，你的输出应形如：\n选择：C\n原因：blahblah blahblah\n\n',
                                role='HUMAN'),
                            ]),
                    type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
                type='opencompass.openicl.icl_evaluator.LMEvaluator'),
            pred_role='BOT'),
        infer_cfg=dict(
            inferencer=dict(
                max_out_len=2048,
                max_seq_len=4096,
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                template=dict(
                    round=[
                        dict(prompt='{question}',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                type='opencompass.openicl.icl_retriever.ZeroRetriever')),
        infer_order='double',
        mode='m2n',
        name='reason_v2',
        path='data/subjective/compass_arena',
        reader_cfg=dict(
            input_columns=[
                'question',
                'ref',
                ],
            output_column='judge'),
        summarizer=dict(
            summary_type='half_add',
            type='opencompass.summarizers.CompassArenaSummarizer'),
        type='opencompass.datasets.CompassArenaDataset'),
    dict(abbr='compassarena_math_v2',
        base_models=[
            dict(abbr='gpt4-turbo',
                type='opencompass.models.openai_api.OpenAI'),
            ],
        eval_cfg=dict(
            evaluator=dict(
                prompt_template=dict(
                    template=dict(
                        round=[
                            dict(prompt='\n请根据提供的 评分要求，用户问题，参考答案 以及 相应的两个回答（回答1，回答2），判断两个回答中哪一个更好。\n评分要求（重要性依次递减）:\n1. 更好的回答的答案能和参考答案一致。\n2. 若两个回答的答案都与参考答案不一致，则更好的回答的推理过程应更加合理。\n3. 更好的回答更加符合与人类对话的习惯，包括语气、情调等。\n\n[用户问题]\n{question}\n\n[参考答案]\n{ref}\n\n\n[回答1开始]\n{prediction}\n[回答1结束]\n\n[回答2开始]\n{prediction2}\n[回答2结束]\n\n根据评分要求，在以下 3 个选项中做出选择:\nA. 回答1更好\nB. 回答2更好\nC. 回答1、2平局\n并提供你的解释原因。\n\n如果你认为回答1更好，你的输出应形如：\n选择：A\n原因：blahblah blahblah\n\n\n如果你认为回答2更好，你的输出应形如：\n选择：B\n原因：blahblah blahblah\n\n\n如果你认为回答1、2打成平手，你的输出应形如：\n选择：C\n原因：blahblah blahblah\n\n',
                                role='HUMAN'),
                            ]),
                    type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
                type='opencompass.openicl.icl_evaluator.LMEvaluator'),
            pred_role='BOT'),
        infer_cfg=dict(
            inferencer=dict(
                max_out_len=2048,
                max_seq_len=4096,
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                template=dict(
                    round=[
                        dict(prompt='{question}',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                type='opencompass.openicl.icl_retriever.ZeroRetriever')),
        infer_order='double',
        mode='m2n',
        name='math_v2',
        path='data/subjective/compass_arena',
        reader_cfg=dict(
            input_columns=[
                'question',
                'ref',
                ],
            output_column='judge'),
        summarizer=dict(
            summary_type='half_add',
            type='opencompass.summarizers.CompassArenaSummarizer'),
        type='opencompass.datasets.CompassArenaDataset'),
    dict(abbr='compassarena_creationv2_zh',
        base_models=[
            dict(abbr='gpt4-turbo',
                type='opencompass.models.openai_api.OpenAI'),
            ],
        eval_cfg=dict(
            evaluator=dict(
                prompt_template=dict(
                    template=dict(
                        round=[
                            dict(prompt='\n请根据提供的 评分要求，用户问题 以及 相应的两个回答（回答1，回答2），判断两个回答中哪一个更好。\n评分要求（重要性依次递减）:\n1. 好的回答必须首先符合用户问题里的各种需求，不能跑题\n2. 好的回答必须具有逻辑连贯性，围绕一个中心进行回答\n3. 好的回答必须具有创造性的词语和表达丰富度\n\n[用户问题]\n{question}\n\n\n[回答1开始]\n{prediction}\n[回答1结束]\n\n[回答2开始]\n{prediction2}\n[回答2结束]\n\n根据评分要求，在以下 3 个选项中做出选择:\nA. 回答1更好\nB. 回答2更好\nC. 回答1、2平局\n并提供你的解释原因。\n\n如果你认为回答1更好，你的输出应形如：\n选择：A\n原因：blahblah blahblah\n\n\n如果你认为回答2更好，你的输出应形如：\n选择：B\n原因：blahblah blahblah\n\n\n如果你认为回答1、2打成平手，你的输出应形如：\n选择：C\n原因：blahblah blahblah\n\n',
                                role='HUMAN'),
                            ]),
                    type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
                type='opencompass.openicl.icl_evaluator.LMEvaluator'),
            pred_role='BOT'),
        infer_cfg=dict(
            inferencer=dict(
                max_out_len=2048,
                max_seq_len=4096,
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                template=dict(
                    round=[
                        dict(prompt='{question}',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                type='opencompass.openicl.icl_retriever.ZeroRetriever')),
        infer_order='double',
        mode='m2n',
        name='creationv2_zh',
        path='data/subjective/compass_arena',
        reader_cfg=dict(
            input_columns=[
                'question',
                'ref',
                ],
            output_column='judge'),
        summarizer=dict(
            summary_type='half_add',
            type='opencompass.summarizers.CompassArenaSummarizer'),
        type='opencompass.datasets.CompassArenaDataset'),
    ]
compassbench_datasets=[
    dict(abbr='CompassBenchV1.1',
        base_models=[
            dict(abbr='gpt4-turbo',
                type='opencompass.models.openai_api.OpenAI'),
            ],
        eval_cfg=dict(
            evaluator=dict(
                prompt_template=dict(
                    template=dict(
                        round=[
                            dict(prompt='{judge_prompt}',
                                role='HUMAN'),
                            ]),
                    type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
                type='opencompass.openicl.icl_evaluator.LMEvaluator'),
            pred_role='BOT'),
        infer_cfg=dict(
            inferencer=dict(
                max_out_len=2048,
                max_seq_len=4096,
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                template=dict(
                    round=[
                        dict(prompt='{question}',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                type='opencompass.openicl.icl_retriever.ZeroRetriever')),
        infer_order='double',
        mode='m2n',
        name='CompassBenchV1.1',
        path='data/subjective/compassbench',
        reader_cfg=dict(
            input_columns=[
                'question',
                'judge_prompt',
                ],
            output_column='judge'),
        summarizer=dict(
            summary_type='half_add',
            type='opencompass.summarizers.CompassBenchSummarizer'),
        type='opencompass.datasets.CompassBenchDataset'),
    ]
datasets=[
    dict(abbr='alignment_bench',
        alignment_bench_config_name='multi-dimension',
        alignment_bench_config_path='data/subjective/alignment_bench/config',
        eval_cfg=dict(
            evaluator=dict(
                prompt_template=dict(
                    template=dict(
                        round=[
                            dict(prompt='{critiquellm_prefix}[助手的答案开始]\n{prediction}\n[助手的答案结束]\n',
                                role='HUMAN'),
                            ]),
                    type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
                type='opencompass.openicl.icl_evaluator.LMEvaluator'),
            pred_role='BOT'),
        infer_cfg=dict(
            inferencer=dict(
                max_out_len=2048,
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                template=dict(
                    round=[
                        dict(prompt='{question}',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                type='opencompass.openicl.icl_retriever.ZeroRetriever')),
        mode='singlescore',
        name='alignment_bench',
        path='data/subjective/alignment_bench',
        reader_cfg=dict(
            input_columns=[
                'question',
                'capability',
                'critiquellm_prefix',
                ],
            output_column='judge'),
        summarizer=dict(
            judge_type='general',
            type='opencompass.summarizers.AlignmentBenchSummarizer'),
        type='opencompass.datasets.AlignmentBenchDataset'),
    dict(abbr='alpaca_eval',
        base_models=[
            dict(abbr='gpt4-turbo',
                type='opencompass.models.openai_api.OpenAI'),
            ],
        eval_cfg=dict(
            evaluator=dict(
                prompt_template=dict(
                    template=dict(
                        begin=[
                            dict(fallback_role='HUMAN',
                                prompt='You are a highly efficient assistant, who evaluates and selects the best large language model (LLMs) based on the quality of their responses to a given instruction. This process will be used to create a leaderboard reflecting the most accurate and human-preferred answers.',
                                role='SYSTEM'),
                            ],
                        round=[
                            dict(prompt='\nI require a leaderboard for various large language models. I\'ll provide you with prompts given to these models and their corresponding outputs. Your task is to assess these responses, and select the model that produces the best output from a human perspective.\n\n## Instruction\n\n{\n    "instruction": "{question}",\n}\n\n## Model Outputs\n\nHere are the unordered outputs from the models. Each output is associated with a specific model, identified by a unique model identifier.\n\n{\n    {\n        "model_identifier": "m",\n        "output": "{prediction}"\n    },\n    {\n        "model_identifier": "M",\n        "output": "{prediction2}"\n    }\n}\n\n## Task\n\nEvaluate the models based on the quality and relevance of their outputs, and select the model that generated the best output. Answer by providing the model identifier of the best model. We will use your output as the name of the best model, so make sure your output only contains one of the following model identifiers and nothing else (no quotes, no spaces, no new lines, ...): m or M.\n\n## Best Model Identifier\n',
                                role='HUMAN'),
                            ]),
                    type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
                type='opencompass.openicl.icl_evaluator.LMEvaluator'),
            pred_role='BOT'),
        infer_cfg=dict(
            inferencer=dict(
                max_out_len=4096,
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                template=dict(
                    round=[
                        dict(prompt='{question}',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                type='opencompass.openicl.icl_retriever.ZeroRetriever')),
        infer_order='random',
        mode='m2n',
        name='alpaca_eval',
        path='./data/subjective/alpaca_eval',
        reader_cfg=dict(
            input_columns=[
                'question',
                ],
            output_column='judge'),
        summarizer=dict(
            judge_type='v2',
            type='opencompass.summarizers.AlpacaSummarizer'),
        type='opencompass.datasets.SubjectiveCmpDataset'),
    dict(abbr='arenahard',
        base_models=[
            dict(abbr='gpt4-0314',
                type='opencompass.models.openai_api.OpenAI'),
            ],
        eval_cfg=dict(
            evaluator=dict(
                prompt_template=dict(
                    template=dict(
                        begin=[
                            dict(fallback_role='HUMAN',
                                prompt='Please act as an impartial judge and evaluate the quality of the responses provided by two AI assistants to the user prompt displayed below. You will be given assistant A\'s answer and assistant B\'s answer. Your job is to evaluate which assistant\'s answer is better.\n\nBegin your evaluation by generating your own answer to the prompt. You must provide your answers before judging any answers.\n\nWhen evaluating the assistants\' answers, compare both assistants\' answers with your answer. You must identify and correct any mistakes or inaccurate information.\n\nThen consider if the assistant\'s answers are helpful, relevant, and concise. Helpful means the answer correctly responds to the prompt or follows the instructions. Note when user prompt has any ambiguity or more than one interpretation, it is more helpful and appropriate to ask for clarifications or more information from the user than providing an answer based on assumptions. Relevant means all parts of the response closely connect or are appropriate to what is being asked. Concise means the response is clear and not verbose or excessive.\n\nThen consider the creativity and novelty of the assistant\'s answers when needed. Finally, identify any missing important information in the assistants\' answers that would be beneficial to include when responding to the user prompt.\n\nAfter providing your explanation, you must output only one of the following choices as your final verdict with a label:\n\n1. Assistant A is significantly better: [[A>>B]]\n2. Assistant A is slightly better: [[A>B]]\n3. Tie, relatively the same: [[A=B]]\n4. Assistant B is slightly better: [[B>A]]\n5. Assistant B is significantly better: [[B>>A]]\n\nExample output: "My final verdict is tie: [[A=B]]".',
                                role='SYSTEM'),
                            ],
                        round=[
                            dict(prompt="<|User Prompt|>\n{question}\n\n<|The Start of Assistant A's Answer|>\n{prediction}\n<|The End of Assistant A's Answer|>\n\n<|The Start of Assistant B's Answer|>\n{prediction2}\n<|The End of Assistant B's Answer|>",
                                role='HUMAN'),
                            ]),
                    type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
                type='opencompass.openicl.icl_evaluator.LMEvaluator'),
            pred_role='BOT'),
        infer_cfg=dict(
            inferencer=dict(
                max_out_len=4096,
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                template=dict(
                    round=[
                        dict(prompt='{question}',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                type='opencompass.openicl.icl_retriever.ZeroRetriever')),
        infer_order='double',
        mode='m2n',
        name='question',
        path='./data/subjective/arena_hard',
        reader_cfg=dict(
            input_columns=[
                'question',
                ],
            output_column='judge'),
        summarizer=dict(
            type='opencompass.summarizers.ArenaHardSummarizer'),
        type='opencompass.datasets.ArenaHardDataset'),
    dict(abbr='compassarena_language',
        base_models=[
            dict(abbr='gpt4-turbo',
                type='opencompass.models.openai_api.OpenAI'),
            ],
        eval_cfg=dict(
            evaluator=dict(
                prompt_template=dict(
                    template=dict(
                        round=[
                            dict(prompt='\n请根据提供的 评分要求，用户问题 以及 相应的两个回答（回答1，回答2），判断两个回答中哪一个更好。\n评分要求（重要性依次递减）:\n1. 在有明确的参考答案的情况下，越贴近参考答案或表明了参考答案的意思的回答越好。\n2. 更好的回答在语言表达上更流畅，更加符合与人类对话的习惯，包括语气、情调等\n3. 在都准确答对问题的前提下，更好的回答能进行额外补充，且补充的内容准确无误。\n\n[用户问题]\n{question}\n\n[参考答案]\n{ref}\n\n\n[回答1开始]\n{prediction}\n[回答1结束]\n\n[回答2开始]\n{prediction2}\n[回答2结束]\n\n根据评分要求，在以下 3 个选项中做出选择:\nA. 回答1更好\nB. 回答2更好\nC. 回答1、2平局\n并提供你的解释原因。\n\n如果你认为回答1更好，你的输出应形如：\n选择：A\n原因：blahblah blahblah\n\n\n如果你认为回答2更好，你的输出应形如：\n选择：B\n原因：blahblah blahblah\n\n\n如果你认为回答1、2打成平手，你的输出应形如：\n选择：C\n原因：blahblah blahblah\n\n',
                                role='HUMAN'),
                            ]),
                    type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
                type='opencompass.openicl.icl_evaluator.LMEvaluator'),
            pred_role='BOT'),
        infer_cfg=dict(
            inferencer=dict(
                max_out_len=2048,
                max_seq_len=4096,
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                template=dict(
                    round=[
                        dict(prompt='{question}',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                type='opencompass.openicl.icl_retriever.ZeroRetriever')),
        infer_order='double',
        mode='m2n',
        name='language',
        path='data/subjective/compass_arena',
        reader_cfg=dict(
            input_columns=[
                'question',
                'ref',
                ],
            output_column='judge'),
        summarizer=dict(
            summary_type='half_add',
            type='opencompass.summarizers.CompassArenaSummarizer'),
        type='opencompass.datasets.CompassArenaDataset'),
    dict(abbr='compassarena_knowledge',
        base_models=[
            dict(abbr='gpt4-turbo',
                type='opencompass.models.openai_api.OpenAI'),
            ],
        eval_cfg=dict(
            evaluator=dict(
                prompt_template=dict(
                    template=dict(
                        round=[
                            dict(prompt='\n请根据提供的 评分要求，用户问题，参考答案 以及 相应的两个回答（回答1，回答2），判断两个回答中哪一个更好。\n评分要求（重要性依次递减）:\n1. 更好的回答能与参考答案吻合或表明参考答案的意思。\n2. 在都准确答对问题的前提下，更好的回答能对知识点进行额外补充，且补充的知识准确无误。\n3. 更好的回答更加符合与人类对话的习惯，包括语气、情调等。\n\n[用户问题]\n{question}\n\n[参考答案]\n{ref}\n\n\n[回答1开始]\n{prediction}\n[回答1结束]\n\n[回答2开始]\n{prediction2}\n[回答2结束]\n\n根据评分要求，在以下 3 个选项中做出选择:\nA. 回答1更好\nB. 回答2更好\nC. 回答1、2平局\n并提供你的解释原因。\n\n如果你认为回答1更好，你的输出应形如：\n选择：A\n原因：blahblah blahblah\n\n\n如果你认为回答2更好，你的输出应形如：\n选择：B\n原因：blahblah blahblah\n\n\n如果你认为回答1、2打成平手，你的输出应形如：\n选择：C\n原因：blahblah blahblah\n\n',
                                role='HUMAN'),
                            ]),
                    type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
                type='opencompass.openicl.icl_evaluator.LMEvaluator'),
            pred_role='BOT'),
        infer_cfg=dict(
            inferencer=dict(
                max_out_len=2048,
                max_seq_len=4096,
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                template=dict(
                    round=[
                        dict(prompt='{question}',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                type='opencompass.openicl.icl_retriever.ZeroRetriever')),
        infer_order='double',
        mode='m2n',
        name='knowledge',
        path='data/subjective/compass_arena',
        reader_cfg=dict(
            input_columns=[
                'question',
                'ref',
                ],
            output_column='judge'),
        summarizer=dict(
            summary_type='half_add',
            type='opencompass.summarizers.CompassArenaSummarizer'),
        type='opencompass.datasets.CompassArenaDataset'),
    dict(abbr='compassarena_reason_v2',
        base_models=[
            dict(abbr='gpt4-turbo',
                type='opencompass.models.openai_api.OpenAI'),
            ],
        eval_cfg=dict(
            evaluator=dict(
                prompt_template=dict(
                    template=dict(
                        round=[
                            dict(prompt='\n请根据提供的 评分要求，用户问题，参考答案 以及 相应的两个回答（回答1，回答2），判断两个回答中哪一个更好。\n评分要求（重要性依次递减）:\n1. 更好的回答的答案能和参考答案一致。\n2. 若两个回答的答案都与参考答案不一致，则更好的回答的推理过程应更加合理。\n3. 更好的回答更加符合与人类对话的习惯，包括语气、情调等。\n\n[用户问题]\n{question}\n\n[参考答案]\n{ref}\n\n\n[回答1开始]\n{prediction}\n[回答1结束]\n\n[回答2开始]\n{prediction2}\n[回答2结束]\n\n根据评分要求，在以下 3 个选项中做出选择:\nA. 回答1更好\nB. 回答2更好\nC. 回答1、2平局\n并提供你的解释原因。\n\n如果你认为回答1更好，你的输出应形如：\n选择：A\n原因：blahblah blahblah\n\n\n如果你认为回答2更好，你的输出应形如：\n选择：B\n原因：blahblah blahblah\n\n\n如果你认为回答1、2打成平手，你的输出应形如：\n选择：C\n原因：blahblah blahblah\n\n',
                                role='HUMAN'),
                            ]),
                    type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
                type='opencompass.openicl.icl_evaluator.LMEvaluator'),
            pred_role='BOT'),
        infer_cfg=dict(
            inferencer=dict(
                max_out_len=2048,
                max_seq_len=4096,
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                template=dict(
                    round=[
                        dict(prompt='{question}',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                type='opencompass.openicl.icl_retriever.ZeroRetriever')),
        infer_order='double',
        mode='m2n',
        name='reason_v2',
        path='data/subjective/compass_arena',
        reader_cfg=dict(
            input_columns=[
                'question',
                'ref',
                ],
            output_column='judge'),
        summarizer=dict(
            summary_type='half_add',
            type='opencompass.summarizers.CompassArenaSummarizer'),
        type='opencompass.datasets.CompassArenaDataset'),
    dict(abbr='compassarena_math_v2',
        base_models=[
            dict(abbr='gpt4-turbo',
                type='opencompass.models.openai_api.OpenAI'),
            ],
        eval_cfg=dict(
            evaluator=dict(
                prompt_template=dict(
                    template=dict(
                        round=[
                            dict(prompt='\n请根据提供的 评分要求，用户问题，参考答案 以及 相应的两个回答（回答1，回答2），判断两个回答中哪一个更好。\n评分要求（重要性依次递减）:\n1. 更好的回答的答案能和参考答案一致。\n2. 若两个回答的答案都与参考答案不一致，则更好的回答的推理过程应更加合理。\n3. 更好的回答更加符合与人类对话的习惯，包括语气、情调等。\n\n[用户问题]\n{question}\n\n[参考答案]\n{ref}\n\n\n[回答1开始]\n{prediction}\n[回答1结束]\n\n[回答2开始]\n{prediction2}\n[回答2结束]\n\n根据评分要求，在以下 3 个选项中做出选择:\nA. 回答1更好\nB. 回答2更好\nC. 回答1、2平局\n并提供你的解释原因。\n\n如果你认为回答1更好，你的输出应形如：\n选择：A\n原因：blahblah blahblah\n\n\n如果你认为回答2更好，你的输出应形如：\n选择：B\n原因：blahblah blahblah\n\n\n如果你认为回答1、2打成平手，你的输出应形如：\n选择：C\n原因：blahblah blahblah\n\n',
                                role='HUMAN'),
                            ]),
                    type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
                type='opencompass.openicl.icl_evaluator.LMEvaluator'),
            pred_role='BOT'),
        infer_cfg=dict(
            inferencer=dict(
                max_out_len=2048,
                max_seq_len=4096,
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                template=dict(
                    round=[
                        dict(prompt='{question}',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                type='opencompass.openicl.icl_retriever.ZeroRetriever')),
        infer_order='double',
        mode='m2n',
        name='math_v2',
        path='data/subjective/compass_arena',
        reader_cfg=dict(
            input_columns=[
                'question',
                'ref',
                ],
            output_column='judge'),
        summarizer=dict(
            summary_type='half_add',
            type='opencompass.summarizers.CompassArenaSummarizer'),
        type='opencompass.datasets.CompassArenaDataset'),
    dict(abbr='compassarena_creationv2_zh',
        base_models=[
            dict(abbr='gpt4-turbo',
                type='opencompass.models.openai_api.OpenAI'),
            ],
        eval_cfg=dict(
            evaluator=dict(
                prompt_template=dict(
                    template=dict(
                        round=[
                            dict(prompt='\n请根据提供的 评分要求，用户问题 以及 相应的两个回答（回答1，回答2），判断两个回答中哪一个更好。\n评分要求（重要性依次递减）:\n1. 好的回答必须首先符合用户问题里的各种需求，不能跑题\n2. 好的回答必须具有逻辑连贯性，围绕一个中心进行回答\n3. 好的回答必须具有创造性的词语和表达丰富度\n\n[用户问题]\n{question}\n\n\n[回答1开始]\n{prediction}\n[回答1结束]\n\n[回答2开始]\n{prediction2}\n[回答2结束]\n\n根据评分要求，在以下 3 个选项中做出选择:\nA. 回答1更好\nB. 回答2更好\nC. 回答1、2平局\n并提供你的解释原因。\n\n如果你认为回答1更好，你的输出应形如：\n选择：A\n原因：blahblah blahblah\n\n\n如果你认为回答2更好，你的输出应形如：\n选择：B\n原因：blahblah blahblah\n\n\n如果你认为回答1、2打成平手，你的输出应形如：\n选择：C\n原因：blahblah blahblah\n\n',
                                role='HUMAN'),
                            ]),
                    type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
                type='opencompass.openicl.icl_evaluator.LMEvaluator'),
            pred_role='BOT'),
        infer_cfg=dict(
            inferencer=dict(
                max_out_len=2048,
                max_seq_len=4096,
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                template=dict(
                    round=[
                        dict(prompt='{question}',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                type='opencompass.openicl.icl_retriever.ZeroRetriever')),
        infer_order='double',
        mode='m2n',
        name='creationv2_zh',
        path='data/subjective/compass_arena',
        reader_cfg=dict(
            input_columns=[
                'question',
                'ref',
                ],
            output_column='judge'),
        summarizer=dict(
            summary_type='half_add',
            type='opencompass.summarizers.CompassArenaSummarizer'),
        type='opencompass.datasets.CompassArenaDataset'),
    dict(abbr='CompassBenchV1.1',
        base_models=[
            dict(abbr='gpt4-turbo',
                type='opencompass.models.openai_api.OpenAI'),
            ],
        eval_cfg=dict(
            evaluator=dict(
                prompt_template=dict(
                    template=dict(
                        round=[
                            dict(prompt='{judge_prompt}',
                                role='HUMAN'),
                            ]),
                    type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
                type='opencompass.openicl.icl_evaluator.LMEvaluator'),
            pred_role='BOT'),
        infer_cfg=dict(
            inferencer=dict(
                max_out_len=2048,
                max_seq_len=4096,
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                template=dict(
                    round=[
                        dict(prompt='{question}',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                type='opencompass.openicl.icl_retriever.ZeroRetriever')),
        infer_order='double',
        mode='m2n',
        name='CompassBenchV1.1',
        path='data/subjective/compassbench',
        reader_cfg=dict(
            input_columns=[
                'question',
                'judge_prompt',
                ],
            output_column='judge'),
        summarizer=dict(
            summary_type='half_add',
            type='opencompass.summarizers.CompassBenchSummarizer'),
        type='opencompass.datasets.CompassBenchDataset'),
    dict(abbr='fofo_test_prompts',
        eval_cfg=dict(
            evaluator=dict(
                prompt_template=dict(
                    template=dict(
                        begin=[
                            dict(fallback_role='HUMAN',
                                prompt="You are a helpful assistant who evaluates the correctness and quality of models' outputs.",
                                role='SYSTEM'),
                            ],
                        round=[
                            dict(prompt='\nI would like you to create a leaderboard that evaluates the correctness of the format of answers from various large language models. To accomplish this, you will need to analyze the text prompts given to the models and their corresponding answers. Specifically, please ensure that your evaluation outputs are properly formatted as a json string. I will provide both the prompts and the responses for this purpose.\n\nHere is the prompt:\n{\n    "instruction": "{question}",\n}\n\nHere are the outputs of the models:\n[\n    {\n        "model": "model",\n        "answer": "{prediction}"\n    },\n]\n\nPlease evaluate the formatting of the model\'s responses by checking if they comply with the format specifications stated in the prompt. Perform a thorough format check and provide a detailed explanation for why the format is correct or incorrect. Your feedback should include the name of the model, followed by the format correctness status represented as \'1\' for correct and \'0\' for incorrect. Present your reasoning as bullet points within a single string for each model assessed. In other words, you should produce the following output:\n```json\n[\n    {\n        \'model\': <model-name>,\n        \'format_correctness\': <correctness>,\n        \'reasons\': <reasons-of-format-correctness>\n    }\n]\n```\n\nPlease note that your response should be a properly formatted JSON string and should not contain any additional content. We will load it directly as a JSON string in Python.\n',
                                role='HUMAN'),
                            ]),
                    type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
                type='opencompass.openicl.icl_evaluator.LMEvaluator'),
            pred_role='BOT'),
        infer_cfg=dict(
            inferencer=dict(
                max_out_len=4096,
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                template=dict(
                    round=[
                        dict(prompt='{question}',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                type='opencompass.openicl.icl_retriever.ZeroRetriever')),
        mode='singlescore',
        name='fofo_test_prompts',
        path='./data/subjective/fofo',
        reader_cfg=dict(
            input_columns=[
                'question',
                ],
            output_column='judge'),
        summarizer=dict(
            judge_type='general',
            type='opencompass.summarizers.FofoSummarizer'),
        type='opencompass.datasets.FofoDataset'),
    dict(abbr='fofo_test_prompts_cn',
        eval_cfg=dict(
            evaluator=dict(
                prompt_template=dict(
                    template=dict(
                        begin=[
                            dict(fallback_role='HUMAN',
                                prompt="You are a helpful assistant who evaluates the correctness and quality of models' outputs.",
                                role='SYSTEM'),
                            ],
                        round=[
                            dict(prompt='\nI would like you to create a leaderboard that evaluates the correctness of the format of answers from various large language models. To accomplish this, you will need to analyze the text prompts given to the models and their corresponding answers. Specifically, please ensure that your evaluation outputs are properly formatted as a json string. I will provide both the prompts and the responses for this purpose.\n\nHere is the prompt:\n{\n    "instruction": "{question}",\n}\n\nHere are the outputs of the models:\n[\n    {\n        "model": "model",\n        "answer": "{prediction}"\n    },\n]\n\nPlease evaluate the formatting of the model\'s responses by checking if they comply with the format specifications stated in the prompt. Perform a thorough format check and provide a detailed explanation for why the format is correct or incorrect. Your feedback should include the name of the model, followed by the format correctness status represented as \'1\' for correct and \'0\' for incorrect. Present your reasoning as bullet points within a single string for each model assessed. In other words, you should produce the following output:\n```json\n[\n    {\n        \'model\': <model-name>,\n        \'format_correctness\': <correctness>,\n        \'reasons\': <reasons-of-format-correctness>\n    }\n]\n```\n\nPlease note that your response should be a properly formatted JSON string and should not contain any additional content. We will load it directly as a JSON string in Python.\n',
                                role='HUMAN'),
                            ]),
                    type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
                type='opencompass.openicl.icl_evaluator.LMEvaluator'),
            pred_role='BOT'),
        infer_cfg=dict(
            inferencer=dict(
                max_out_len=4096,
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                template=dict(
                    round=[
                        dict(prompt='{question}',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                type='opencompass.openicl.icl_retriever.ZeroRetriever')),
        mode='singlescore',
        name='fofo_test_prompts_cn',
        path='./data/subjective/fofo',
        reader_cfg=dict(
            input_columns=[
                'question',
                ],
            output_column='judge'),
        summarizer=dict(
            judge_type='general',
            type='opencompass.summarizers.FofoSummarizer'),
        type='opencompass.datasets.FofoDataset'),
    dict(abbr='mtbench_0.0',
        eval_cfg=dict(
            evaluator=dict(
                prompt_template=dict(
                    template=dict(
                        begin=[
                            dict(fallback_role='HUMAN',
                                prompt='{system_prompt}',
                                role='SYSTEM'),
                            ],
                        round=[
                            dict(prompt='{prompt_template}',
                                role='HUMAN'),
                            ]),
                    type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
                type='opencompass.openicl.icl_evaluator.LMEvaluator'),
            pred_role='BOT'),
        infer_cfg=dict(
            inferencer=dict(
                do_sample=False,
                infer_mode='every',
                max_out_len=512,
                max_seq_len=4096,
                temperature=0.0,
                type='opencompass.openicl.icl_inferencer.ChatInferencer'),
            prompt_template=dict(
                template='{dialogue}',
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                type='opencompass.openicl.icl_retriever.ZeroRetriever')),
        mode='singlescore',
        name='mtbench_0.0',
        path='data/subjective/mtbench',
        reader_cfg=dict(
            input_columns=[
                'dialogue',
                'capability',
                'system_prompt',
                'prompt_template',
                ],
            output_column='judge'),
        summarizer=dict(
            judge_type='single',
            type='opencompass.summarizers.MTBenchSummarizer'),
        type='opencompass.datasets.MTBenchDataset'),
    dict(abbr='mtbench_0.1',
        eval_cfg=dict(
            evaluator=dict(
                prompt_template=dict(
                    template=dict(
                        begin=[
                            dict(fallback_role='HUMAN',
                                prompt='{system_prompt}',
                                role='SYSTEM'),
                            ],
                        round=[
                            dict(prompt='{prompt_template}',
                                role='HUMAN'),
                            ]),
                    type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
                type='opencompass.openicl.icl_evaluator.LMEvaluator'),
            pred_role='BOT'),
        infer_cfg=dict(
            inferencer=dict(
                do_sample=True,
                infer_mode='every',
                max_out_len=512,
                max_seq_len=4096,
                temperature=0.1,
                type='opencompass.openicl.icl_inferencer.ChatInferencer'),
            prompt_template=dict(
                template='{dialogue}',
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                type='opencompass.openicl.icl_retriever.ZeroRetriever')),
        mode='singlescore',
        name='mtbench_0.1',
        path='data/subjective/mtbench',
        reader_cfg=dict(
            input_columns=[
                'dialogue',
                'capability',
                'system_prompt',
                'prompt_template',
                ],
            output_column='judge'),
        summarizer=dict(
            judge_type='single',
            type='opencompass.summarizers.MTBenchSummarizer'),
        type='opencompass.datasets.MTBenchDataset'),
    dict(abbr='mtbench_0.7',
        eval_cfg=dict(
            evaluator=dict(
                prompt_template=dict(
                    template=dict(
                        begin=[
                            dict(fallback_role='HUMAN',
                                prompt='{system_prompt}',
                                role='SYSTEM'),
                            ],
                        round=[
                            dict(prompt='{prompt_template}',
                                role='HUMAN'),
                            ]),
                    type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
                type='opencompass.openicl.icl_evaluator.LMEvaluator'),
            pred_role='BOT'),
        infer_cfg=dict(
            inferencer=dict(
                do_sample=True,
                infer_mode='every',
                max_out_len=512,
                max_seq_len=4096,
                temperature=0.7,
                type='opencompass.openicl.icl_inferencer.ChatInferencer'),
            prompt_template=dict(
                template='{dialogue}',
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                type='opencompass.openicl.icl_retriever.ZeroRetriever')),
        mode='singlescore',
        name='mtbench_0.7',
        path='data/subjective/mtbench',
        reader_cfg=dict(
            input_columns=[
                'dialogue',
                'capability',
                'system_prompt',
                'prompt_template',
                ],
            output_column='judge'),
        summarizer=dict(
            judge_type='single',
            type='opencompass.summarizers.MTBenchSummarizer'),
        type='opencompass.datasets.MTBenchDataset'),
    dict(abbr='mtbench101',
        eval_cfg=dict(
            evaluator=dict(
                prompt_template=dict(
                    template=dict(
                        begin=[
                            dict(fallback_role='HUMAN',
                                prompt='{system_prompt}',
                                role='SYSTEM'),
                            ],
                        round=[
                            dict(prompt='{prompt_template}',
                                role='HUMAN'),
                            ]),
                    type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
                type='opencompass.openicl.icl_evaluator.LMEvaluator'),
            pred_role='BOT'),
        infer_cfg=dict(
            inferencer=dict(
                infer_mode='last',
                max_out_len=4096,
                max_seq_len=4096,
                type='opencompass.openicl.icl_inferencer.ChatInferencer'),
            prompt_template=dict(
                template='{dialogue}',
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                type='opencompass.openicl.icl_retriever.ZeroRetriever')),
        mode='singlescore',
        name='mtbench101',
        path='data/subjective/',
        reader_cfg=dict(
            input_columns=[
                'dialogue',
                'task',
                'multi_id',
                'turn_id',
                'system_prompt',
                'prompt_template',
                ],
            output_column='judge'),
        summarizer=dict(
            judge_type='single',
            type='opencompass.summarizers.MTBench101Summarizer'),
        type='opencompass.datasets.MTBench101Dataset'),
    ]
eval=dict(
    given_pred=[
        dict(abbr='gpt4-turbo',
            path='/mnt/petrelfs/caomaosong/backup_hwfile/update_subjective/opencompass/data/subjective/gpt4-turbo'),
        dict(abbr='gpt4-0314',
            path='/mnt/petrelfs/caomaosong/backup_hwfile/update_subjective/opencompass/data/subjective/arena_hard'),
        ],
    partitioner=dict(
        judge_models=[
            dict(abbr='chatglm3-6b-hf',
                batch_size=8,
                generation_kwargs=dict(
                    do_sample=True),
                max_out_len=2048,
                max_seq_len=4096,
                meta_template=dict(
                    round=[
                        dict(api_role='HUMAN',
                            role='HUMAN'),
                        dict(api_role='BOT',
                            generate=True,
                            role='BOT'),
                        ]),
                model_kwargs=dict(
                    device_map='auto',
                    trust_remote_code=True),
                path='THUDM/chatglm3-6b',
                run_cfg=dict(
                    num_gpus=1,
                    num_procs=1),
                tokenizer_kwargs=dict(
                    padding_side='left',
                    truncation_side='left',
                    trust_remote_code=True),
                tokenizer_path='THUDM/chatglm3-6b',
                type='opencompass.models.HuggingFaceChatGLM3'),
            ],
        models=[
            dict(abbr='chatglm3-6b-hf',
                batch_size=8,
                generation_kwargs=dict(
                    do_sample=True),
                max_out_len=2048,
                max_seq_len=4096,
                meta_template=dict(
                    round=[
                        dict(api_role='HUMAN',
                            role='HUMAN'),
                        dict(api_role='BOT',
                            generate=True,
                            role='BOT'),
                        ]),
                model_kwargs=dict(
                    device_map='auto',
                    trust_remote_code=True),
                path='THUDM/chatglm3-6b',
                run_cfg=dict(
                    num_gpus=1,
                    num_procs=1),
                tokenizer_kwargs=dict(
                    padding_side='left',
                    truncation_side='left',
                    trust_remote_code=True),
                tokenizer_path='THUDM/chatglm3-6b',
                type='opencompass.models.HuggingFaceChatGLM3'),
            ],
        type='opencompass.partitioners.sub_naive.SubjectiveNaivePartitioner'),
    runner=dict(
        max_num_workers=2,
        task=dict(
            type='opencompass.tasks.subjective_eval.SubjectiveEvalTask'),
        type='opencompass.runners.LocalRunner'))
fofo_datasets=[
    dict(abbr='fofo_test_prompts',
        eval_cfg=dict(
            evaluator=dict(
                prompt_template=dict(
                    template=dict(
                        begin=[
                            dict(fallback_role='HUMAN',
                                prompt="You are a helpful assistant who evaluates the correctness and quality of models' outputs.",
                                role='SYSTEM'),
                            ],
                        round=[
                            dict(prompt='\nI would like you to create a leaderboard that evaluates the correctness of the format of answers from various large language models. To accomplish this, you will need to analyze the text prompts given to the models and their corresponding answers. Specifically, please ensure that your evaluation outputs are properly formatted as a json string. I will provide both the prompts and the responses for this purpose.\n\nHere is the prompt:\n{\n    "instruction": "{question}",\n}\n\nHere are the outputs of the models:\n[\n    {\n        "model": "model",\n        "answer": "{prediction}"\n    },\n]\n\nPlease evaluate the formatting of the model\'s responses by checking if they comply with the format specifications stated in the prompt. Perform a thorough format check and provide a detailed explanation for why the format is correct or incorrect. Your feedback should include the name of the model, followed by the format correctness status represented as \'1\' for correct and \'0\' for incorrect. Present your reasoning as bullet points within a single string for each model assessed. In other words, you should produce the following output:\n```json\n[\n    {\n        \'model\': <model-name>,\n        \'format_correctness\': <correctness>,\n        \'reasons\': <reasons-of-format-correctness>\n    }\n]\n```\n\nPlease note that your response should be a properly formatted JSON string and should not contain any additional content. We will load it directly as a JSON string in Python.\n',
                                role='HUMAN'),
                            ]),
                    type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
                type='opencompass.openicl.icl_evaluator.LMEvaluator'),
            pred_role='BOT'),
        infer_cfg=dict(
            inferencer=dict(
                max_out_len=4096,
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                template=dict(
                    round=[
                        dict(prompt='{question}',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                type='opencompass.openicl.icl_retriever.ZeroRetriever')),
        mode='singlescore',
        name='fofo_test_prompts',
        path='./data/subjective/fofo',
        reader_cfg=dict(
            input_columns=[
                'question',
                ],
            output_column='judge'),
        summarizer=dict(
            judge_type='general',
            type='opencompass.summarizers.FofoSummarizer'),
        type='opencompass.datasets.FofoDataset'),
    dict(abbr='fofo_test_prompts_cn',
        eval_cfg=dict(
            evaluator=dict(
                prompt_template=dict(
                    template=dict(
                        begin=[
                            dict(fallback_role='HUMAN',
                                prompt="You are a helpful assistant who evaluates the correctness and quality of models' outputs.",
                                role='SYSTEM'),
                            ],
                        round=[
                            dict(prompt='\nI would like you to create a leaderboard that evaluates the correctness of the format of answers from various large language models. To accomplish this, you will need to analyze the text prompts given to the models and their corresponding answers. Specifically, please ensure that your evaluation outputs are properly formatted as a json string. I will provide both the prompts and the responses for this purpose.\n\nHere is the prompt:\n{\n    "instruction": "{question}",\n}\n\nHere are the outputs of the models:\n[\n    {\n        "model": "model",\n        "answer": "{prediction}"\n    },\n]\n\nPlease evaluate the formatting of the model\'s responses by checking if they comply with the format specifications stated in the prompt. Perform a thorough format check and provide a detailed explanation for why the format is correct or incorrect. Your feedback should include the name of the model, followed by the format correctness status represented as \'1\' for correct and \'0\' for incorrect. Present your reasoning as bullet points within a single string for each model assessed. In other words, you should produce the following output:\n```json\n[\n    {\n        \'model\': <model-name>,\n        \'format_correctness\': <correctness>,\n        \'reasons\': <reasons-of-format-correctness>\n    }\n]\n```\n\nPlease note that your response should be a properly formatted JSON string and should not contain any additional content. We will load it directly as a JSON string in Python.\n',
                                role='HUMAN'),
                            ]),
                    type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
                type='opencompass.openicl.icl_evaluator.LMEvaluator'),
            pred_role='BOT'),
        infer_cfg=dict(
            inferencer=dict(
                max_out_len=4096,
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                template=dict(
                    round=[
                        dict(prompt='{question}',
                            role='HUMAN'),
                        ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                type='opencompass.openicl.icl_retriever.ZeroRetriever')),
        mode='singlescore',
        name='fofo_test_prompts_cn',
        path='./data/subjective/fofo',
        reader_cfg=dict(
            input_columns=[
                'question',
                ],
            output_column='judge'),
        summarizer=dict(
            judge_type='general',
            type='opencompass.summarizers.FofoSummarizer'),
        type='opencompass.datasets.FofoDataset'),
    ]
infer=dict(
    partitioner=dict(
        type='opencompass.partitioners.NaivePartitioner'),
    runner=dict(
        max_num_workers=2,
        task=dict(
            type='opencompass.tasks.OpenICLInferTask'),
        type='opencompass.runners.LocalRunner'))
judge_models=[
    dict(abbr='chatglm3-6b-hf',
        batch_size=8,
        generation_kwargs=dict(
            do_sample=True),
        max_out_len=2048,
        max_seq_len=4096,
        meta_template=dict(
            round=[
                dict(api_role='HUMAN',
                    role='HUMAN'),
                dict(api_role='BOT',
                    generate=True,
                    role='BOT'),
                ]),
        model_kwargs=dict(
            device_map='auto',
            trust_remote_code=True),
        path='THUDM/chatglm3-6b',
        run_cfg=dict(
            num_gpus=1,
            num_procs=1),
        tokenizer_kwargs=dict(
            padding_side='left',
            truncation_side='left',
            trust_remote_code=True),
        tokenizer_path='THUDM/chatglm3-6b',
        type='opencompass.models.HuggingFaceChatGLM3'),
    ]
models=[
    dict(abbr='chatglm3-6b-hf',
        batch_size=8,
        generation_kwargs=dict(
            do_sample=True),
        max_out_len=2048,
        max_seq_len=4096,
        meta_template=dict(
            round=[
                dict(api_role='HUMAN',
                    role='HUMAN'),
                dict(api_role='BOT',
                    generate=True,
                    role='BOT'),
                ]),
        model_kwargs=dict(
            device_map='auto',
            trust_remote_code=True),
        path='THUDM/chatglm3-6b',
        run_cfg=dict(
            num_gpus=1,
            num_procs=1),
        tokenizer_kwargs=dict(
            padding_side='left',
            truncation_side='left',
            trust_remote_code=True),
        tokenizer_path='THUDM/chatglm3-6b',
        type='opencompass.models.HuggingFaceChatGLM3'),
    ]
mtbench101_datasets=[
    dict(abbr='mtbench101',
        eval_cfg=dict(
            evaluator=dict(
                prompt_template=dict(
                    template=dict(
                        begin=[
                            dict(fallback_role='HUMAN',
                                prompt='{system_prompt}',
                                role='SYSTEM'),
                            ],
                        round=[
                            dict(prompt='{prompt_template}',
                                role='HUMAN'),
                            ]),
                    type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
                type='opencompass.openicl.icl_evaluator.LMEvaluator'),
            pred_role='BOT'),
        infer_cfg=dict(
            inferencer=dict(
                infer_mode='last',
                max_out_len=4096,
                max_seq_len=4096,
                type='opencompass.openicl.icl_inferencer.ChatInferencer'),
            prompt_template=dict(
                template='{dialogue}',
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                type='opencompass.openicl.icl_retriever.ZeroRetriever')),
        mode='singlescore',
        name='mtbench101',
        path='data/subjective/',
        reader_cfg=dict(
            input_columns=[
                'dialogue',
                'task',
                'multi_id',
                'turn_id',
                'system_prompt',
                'prompt_template',
                ],
            output_column='judge'),
        summarizer=dict(
            judge_type='single',
            type='opencompass.summarizers.MTBench101Summarizer'),
        type='opencompass.datasets.MTBench101Dataset'),
    ]
mtbench_datasets=[
    dict(abbr='mtbench_0.0',
        eval_cfg=dict(
            evaluator=dict(
                prompt_template=dict(
                    template=dict(
                        begin=[
                            dict(fallback_role='HUMAN',
                                prompt='{system_prompt}',
                                role='SYSTEM'),
                            ],
                        round=[
                            dict(prompt='{prompt_template}',
                                role='HUMAN'),
                            ]),
                    type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
                type='opencompass.openicl.icl_evaluator.LMEvaluator'),
            pred_role='BOT'),
        infer_cfg=dict(
            inferencer=dict(
                do_sample=False,
                infer_mode='every',
                max_out_len=512,
                max_seq_len=4096,
                temperature=0.0,
                type='opencompass.openicl.icl_inferencer.ChatInferencer'),
            prompt_template=dict(
                template='{dialogue}',
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                type='opencompass.openicl.icl_retriever.ZeroRetriever')),
        mode='singlescore',
        name='mtbench_0.0',
        path='data/subjective/mtbench',
        reader_cfg=dict(
            input_columns=[
                'dialogue',
                'capability',
                'system_prompt',
                'prompt_template',
                ],
            output_column='judge'),
        summarizer=dict(
            judge_type='single',
            type='opencompass.summarizers.MTBenchSummarizer'),
        type='opencompass.datasets.MTBenchDataset'),
    dict(abbr='mtbench_0.1',
        eval_cfg=dict(
            evaluator=dict(
                prompt_template=dict(
                    template=dict(
                        begin=[
                            dict(fallback_role='HUMAN',
                                prompt='{system_prompt}',
                                role='SYSTEM'),
                            ],
                        round=[
                            dict(prompt='{prompt_template}',
                                role='HUMAN'),
                            ]),
                    type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
                type='opencompass.openicl.icl_evaluator.LMEvaluator'),
            pred_role='BOT'),
        infer_cfg=dict(
            inferencer=dict(
                do_sample=True,
                infer_mode='every',
                max_out_len=512,
                max_seq_len=4096,
                temperature=0.1,
                type='opencompass.openicl.icl_inferencer.ChatInferencer'),
            prompt_template=dict(
                template='{dialogue}',
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                type='opencompass.openicl.icl_retriever.ZeroRetriever')),
        mode='singlescore',
        name='mtbench_0.1',
        path='data/subjective/mtbench',
        reader_cfg=dict(
            input_columns=[
                'dialogue',
                'capability',
                'system_prompt',
                'prompt_template',
                ],
            output_column='judge'),
        summarizer=dict(
            judge_type='single',
            type='opencompass.summarizers.MTBenchSummarizer'),
        type='opencompass.datasets.MTBenchDataset'),
    dict(abbr='mtbench_0.7',
        eval_cfg=dict(
            evaluator=dict(
                prompt_template=dict(
                    template=dict(
                        begin=[
                            dict(fallback_role='HUMAN',
                                prompt='{system_prompt}',
                                role='SYSTEM'),
                            ],
                        round=[
                            dict(prompt='{prompt_template}',
                                role='HUMAN'),
                            ]),
                    type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
                type='opencompass.openicl.icl_evaluator.LMEvaluator'),
            pred_role='BOT'),
        infer_cfg=dict(
            inferencer=dict(
                do_sample=True,
                infer_mode='every',
                max_out_len=512,
                max_seq_len=4096,
                temperature=0.7,
                type='opencompass.openicl.icl_inferencer.ChatInferencer'),
            prompt_template=dict(
                template='{dialogue}',
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                type='opencompass.openicl.icl_retriever.ZeroRetriever')),
        mode='singlescore',
        name='mtbench_0.7',
        path='data/subjective/mtbench',
        reader_cfg=dict(
            input_columns=[
                'dialogue',
                'capability',
                'system_prompt',
                'prompt_template',
                ],
            output_column='judge'),
        summarizer=dict(
            judge_type='single',
            type='opencompass.summarizers.MTBenchSummarizer'),
        type='opencompass.datasets.MTBenchDataset'),
    ]
summarizer=dict(
    function='subjective',
    type='opencompass.summarizers.SubjectiveSummarizer')
work_dir='outputs/subjective/20240703_103406'
